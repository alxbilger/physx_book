#import "variables.typ": *
#import "box.typ": *

= Numerical Integration <section_numerical_integration>

== Ordinary Differential Equation

#definition(title:"Ordingary differential equation")[
  An ordinary differential equation (ODE) is an equation involving a function of one independent variable and its derivatives.
]

#definition(title:"First-order ODE")[
  A first-order ordinary differential equation ODE is an equation involving a function of one independent variable and its first derivative (but no higher-order derivatives).
]

#definition(title:"Explicit first-order ODE")[
  An explicit first-order ordinary differential equation (ODE) is a first-order ODE in the form 
  $
    y' = f(t,y)
  $
$f(t,y)$ is a function of the independent variable $t$
and the dependent variable $y$.
]

#definition(title: "Implicit first-order ODE")[
  An implicit first-order ordinary differential equation (ODE) is a first-order ODE in the form 
  $
    f(t, y, y') = 0
  $
  where $f$ is a function that cannot be explicitly solved for $y'$ (i.e., rearranging to isolate $y'$ is impossible or impractical).
]

#definition(title: "Explicit first-order system of ODEs")[
  An explicit first-order system of ordinary differential equations (ODEs) is a set of coupled first-order ODEs in the form 
  $
    mat(y'_0; y'_1; dots.v; y'_(n-1)) = mat(
      f_0(t, bold(y));
      f_1(t, bold(y));
      dots.v;
      f_(n-1)(t, bold(y));
    )
  $
  where $mat(y_0, y_1, dots, y_(n-1))$ is a vector of dependent variables, 
$t$ is independent variable, and $mat(f_0(t, bold(y)), f_1(t, bold(y)), dots, f_(n-1)(t, bold(y)))$ is a vector-valued function that explicitly solves for the derivatives of all components of $bold(y)$ at each time step.
]

#definition(title: "Implicit first-order system of ODEs")[
  

  An implicit first-order system of ordinary differential equations (ODEs) is a set of coupled first-order ODEs in the form 
  $
    mat(
      f_0(t, bold(y), bold(y)');
      f_1(t, bold(y), bold(y)');
      dots.v;
      f_(n-1)(t, bold(y), bold(y)');
    ) = mat(0; 0; dots.v; 0)
  $
  where $mat(f_0(t, bold(y)), f_1(t, bold(y)), dots, f_(n-1)(t, bold(y)))$ is a vector-valued function such that the derivatives $y'$ cannot be explicitly isolated (i.e., rearranging to solve for $y'$ is impossible or impractical without additional variables).
]

#definition(title: "Initial Value Problem")[
  An initial value problem (IVP) is an ODE together with initial conditions specifying the value of the solution at a single point in the domain of the independent variable.

  Example with a first-order explicit ODE:

  $
  mat(delim: #("{", none),
    y'     &=f(t,y);
    y(t_0) &= y_0
  )
  $ <initial_value_problem>

  where:
  - $y(t)$ is the unknown function to be determined
  - $f(t, y)$ is a given function
  - $t_0$ is the initial time
  - $y_0 = y(t_0)$ is the initial condition
]

As defined in the previous section, the numerical solution at step $n$ is $y_n = y(t_n)$. The IVP provides the *first* value $y_0 = y(t_0)$ to start the computation, and subsequent steps ($y_1, y_2, dots$) are generated by a numerical method (e.g. Linear Multistep Methods).

#definition(title:"Mass matrix ODE")[
  $
  mat(delim: #("{", none),
    odemassmatrix(t,y) thick y'     &=f(t,y);
    y(t_0) &= y_0
  )
  $ <eq_mass_matrix_ode>
]

#definition(title:"Energy ODE")[
  $
    mat(delim: #("{", none),
      odemassmatrix(t,y) thick y' &= -nabla odeenergy(y);
      y(t_0) &= y_0
    )
  $ <eq_energy_ode>
]

== Time-stepping methods

#definition(title:"Time-stepping methods")[
  Time-stepping methods are numerical algorithms that solve time-dependent ODEs (time is the independant variable) by advancing the solution from one discrete time step to the next, using the current step's values (and/or the previous steps values) to compute the next state.
  
  The continuous time domain is discretized into a sequence of equally spaced points. For a given function $y(t)$ with initial condition $y(t_0) = y_0$, we define:

$
t_n = t_0 + n thick Delta t, wide n = 0, 1, 2, dots
$

where $Delta t > 0$ is the time step size. At these discrete points, we compute the function values:

$
y_n=y(t_n)
$ <sequence>

This sequence $y_n$ represents an approximation to the solution of the ODE at the discrete time points $t_n$. 
]

== Linear Multistep Method

Linear multistep methods (LMMs) represent a fundamental class of numerical techniques for approximating solutions to ordinary differential equations (ODEs) of the form of @initial_value_problem. LMMs leverage multiple previous solution values to compute the next step.

The general form of a linear multistep method is defined by the recurrence relation:
$
y_(n+s) + a_(s-1) y_(n+s-1) + a_(s-2) y_(n+s-2) + dots + a_0 y_n = \ stepsize ( b_s f(t_(n+s), y_(n+s)) + b_(s-1) f(t_(n+s-1), y_(n+s-1)) + dots + b_0 f(t_n, y_n))
$

or

$
sum_(j=0)^s a_j y_(n+j) = stepsize sum_(j=0)^s b_j f(t_(n+j),y_(n+j))
$ <linear_multistep_method>

If $b_s = 0$, the method is called "explicit": it is possible to compute $y_(n+s)$ directly.
If $b_s != 0$, the method is called "implicit": the value of $y_(n+s)$ depends on the value of $f(t_(n+s), y_(n+s))$. Those methods require a non-linear solver. An example of such solver is introduced in @section_newton_raphson. 

== Backward Differentiation Formula

Backward Differentiation Formula (BDF) are a subclass of implicit LMMs where the coefficients are computed based on Lagrange interpolation polynomials.

Given a set of $s+1$ nodes ${t_n, t_(n+1), ..., t_(n+s)}$, the Lagrange basis for polynomials of degree $<= s$ for those nodes is the set of polynomials ${l_0(t), l_1(t), ..., l_s (t)}$:

$
l_j (t) = product_(0<=m<=s\ m!=j) (t-t_(n+m))/(t_(n+j)-t_(n+m))
$

The Lagrange interpolating polynomial for those nodes through the corresponding values ${y_(n), y_(n+1), ..., y_(n+s)}$ is the linear combination:

$
L(t) = sum_(j=0)^s y_(n+j) l_j (t)
$

=== Derivative:

$
L'(t) = sum_(j=0)^s y_(n+j) l'_j (t)
$

$
l'_j (t) = sum_(i = 0 \ i != j)^s [ 1 / (t_(n+j) - t_(n+i)) product_(m = 0 \ m != (i,j)) (t - t_(n+m)) / (t_(n+j) - t_(n+m))]
$

=== Lagrange polynomials to solve an ODE
We approximate $y'$ by $L'$ in @initial_value_problem:

$
sum_(j=0)^s y_(n+j) l'_j (t) = f(t,y)
$

We want to find $y(t_(n+s))$, therefore

$
sum_(j=0)^s y_(n+j) l'_j (t_(n+s)) = f(t_(n+s),y_(n+s))
$


$
sum_(j=0)^s y_(n+j) (sum_(i = 0 \ i != j)^s [ 1 / (t_(n+j) - t_(n+i)) product_(m = 0 \ m != (i,j)) (t_(n+s) - t_(n+m)) / (t_(n+j) - t_(n+m))]) = f(t_(n+s),y_(n+s))
$

=== Constant Step Size

$
t_(n+j) = t_n + j thick stepsize
$

So, for all $i,j$

$
t_(n+j) - t_(n+i) 
&= t_n + j thick stepsize - (t_n + i thick stepsize)\
&= (j-i) stepsize
$

$
sum_(j=0)^s y_(n+j) (sum_(i = 0 \ i != j)^s [ 1 / (j - i) product_(m = 0 \ m != (i,j)) (s - m) / (j - m)]) = stepsize thick f(t_(n+s),y_(n+s))
$

==== BDF1

For $s=1$:

$j=0$:
$
l'_0 (t_(n+1)) = sum_(i = 0 \ i != 0)^1 [ 1 / (0 - i) product_(m = 0 \ m != (i,0)) (1 - m) / (- m)] =  1 / (-1) product_(m = 0 \ m !=(1,0)) (1 - m) / (- m) = -1
$

$j=1$:

$
l'_1 (t_(n+1)) = sum_(i = 0 \ i != 1)^1 [ 1 / (1 - i) product_(m = 0 \ m != (i,1)) (1 - m) / (1 - m)] =  1 / (1) product_(m = 0 \ m != (0,1)) (1 - m) / (1 - m) = 1
$

Finally, for $s = 1$:

$
y_(n+1) - y_(n) = stepsize thick f(t_(n+1), y_(n+1))
$

==== BDF2

For $s=2$:

$j = 0$:
$
l'_0 (t_(n+2)) & = sum_(i = 0 \ i != 0)^2 [ 1 / ( - i) product_(m = 0 \ m != (i,0)) (2 - m) / (- m)] \ 
&= ( 1 / ( - 1) product_(m = 0 \ m != (1,0)) (2 - m) / (- m)) + ( 1 / ( - 2) product_(m = 0 \ m != (2,0)) (2 - m) / (- m)) \ 
&= -1/2 ((2-1)/(-1)) = 1/2
$

$j=1$:

$
l'_1 (t_(n+2)) & = sum_(i = 0 \ i != 1)^2 [ 1 / (1 - i) product_(m = 0 \ m != (i,1)) (2 - m) / (1 - m)] \
&= ( 1 / (1) product_(m = 0 \ m != (0,1)) (2 - m) / (1 - m)) + ( 1 / (1 - 2) product_(m = 0 \ m != (2,1)) (2 - m) / (1 - m)) \
&= -2
$

$j=2$:

$
l'_2 (t_(n+2)) & = sum_(i = 0 \ i != 2)^2 [ 1 / (2 - i) product_(m = 0 \ m != (i,2)) (2 - m) / (2 - m)] \
&= ( 1 / (2 ) product_(m = 0 \ m != (0,2)) (2 - m) / (2 - m)) + ( 1 / (2 - 1) product_(m = 0 \ m != (1,2)) (2 - m) / (2 - m))\
&= 3/2
$

Finally, for $s = 1$:

$
3/2 y_(n+2) -2 y_(n+1) + 1/2 y_(n) = stepsize thick f(t_(n+2), y_(n+2))
$

which can be written:

$
y_(n+2) - 4/3 y_(n+1) + 1/3 y_(n) = 2/3 stepsize thick f(t_(n+2), y_(n+2))
$


== Physical laws as ODE

The previous sections established methods for solving IVPs (@initial_value_problem), applicable to any function $y$ and $f$. This section now demonstrates how these methods are applied to specific physical systems, where $y$ and $f$ are explicitly defined by the system's governing dynamics.

=== Newton's Second Law of Motion <section_newton_second_law_as_ODE>

The second Newton's law (@ODE) is a first-order system of ordinary differential equations of the form of @eq_mass_matrix_ode ($odemassmatrix(t,y) thick y'= f(t,y), quad y(t_0) = y_0$) where:

$
y(t) = mat( position(t); velocity(t))
$ <definition_y>

$
f(t,y) = mat(
  velocity(t);
  force(position, velocity) - coriolismatrix velocity
)
$ <definition_f>

and

$
  odemassmatrix(t,y) = mat(
    identity,0;
    0, massmatrix
  )
$

All together, we obtain the following initial value problem:

$
  mat(delim: #("{", none),

  mat(
    identity,0;
    0, massmatrix
  ) thick
  d/(d t) mat( position(t); velocity(t)) &
  =
  mat(
    velocity(t);
    force(position, velocity) - coriolismatrix velocity
  ) ;
  mat( position(t_0); velocity(t_0))& = mat( position_0; velocity_0)
)
$

or

$
  mat(delim: #("{", none),
    mat(
        identity,0;
        0, massmatrix
      ) thick
      (d state)/(d t) &= mat(
    velocity(t);
    force(position, velocity) - coriolismatrix velocity
  );
  state(t_0) &= state_0
  )
$

In case of Rayleigh damping (@F_rayleigh):

$
f(t,y) = mat( velocity(t); massmatrix^(-1) (force(position, velocity) - coriolismatrix velocity + (-alpha massmatrix + beta stiffness(position, velocity)) velocity))
$

=== Heat Equation <section_heat_equation_as_ODE>

@heat_equation is of the form of @initial_value_problem where:

$
y(t) = u(t)
$

$
f(t,y) = diffusivity laplace u
$

== Newton-Raphson <section_newton_raphson>

For implicit methods, @linear_multistep_method is nonlinear. The Newton-Raphson algorithm can be used to solve it.

=== Nonlinear function


The algorithm applies to find the root $x_r$ of a nonlinear function $r:RR^k arrow RR^k$ such that:

$
r(x_r) = 0
$ <nonlinear_equation>

Let's define $x^0$ the first estimate of the solution of this equation, called the initial guess.

$
Delta x^0 = x_r - x_0
$

Taylor series expansion of $r$ around $x^0$

$
r(x_r)&= r(x_0 + Delta x_0) \
&=r(x_0) + lr((partial r)/(partial x)|)_(x^0) Delta x^0 + O(norm(Delta x^0)^2)
$

If we neglect second-order terms and higher:

$
r(x_r) approx r(x_0) + lr((partial r)/(partial x)|)_(x^0) Delta x^0
$

If we use this approximation to solve the equation @nonlinear_equation, it leads to:

$
r(x^0) + lr((partial r)/(partial x)|)_(x^0) Delta x^0 = 0
$

This is a linear system to solve for the unknown $Delta x^0$:

$
lr((partial r)/(partial x)|)_(x^0) Delta x^0 = -r(x^0)
$

Once $Delta x^0$ is found, $x^1$ can be deduced:

$
x^1 = Delta x^0 + x^0
$

The process is repeated as

$
lr((partial r)/(partial x)|)_(x^i) (x^(i+1)-x^i) = -r(x^i)
$ <linear_system_in_newton_raphson>

=== Optimization <newton_raphson_optimization>

Newton-Raphson method can also be used in optimization problems.

Given a twice-differentiable function $r = r(x)$, optimizing $r$ is equivalent to finding the roots of $nabla r = (partial r)/(partial x)$, i.e. solving $nabla r(x)=0$. This can be done using the Newton-Raphson method.

The solution may be a minima, maxima, or saddle point.

Note that applying the Newton-Raphson method on $nabla r$ requires the Hessian matrix $nabla^2 r = (partial^2 r)/(partial x^2)$.

The iteration process is:

$
  lr((partial^2 r)/(partial x^2)|)_(x^i) (x^(i+1)-x^i) = -lr((partial r)/(partial x)|)_(x^i)
$

